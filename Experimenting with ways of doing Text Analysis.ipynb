{
 "metadata": {
  "name": "",
  "signature": "sha256:e2e589f843ef435da4517e8f4b6d6de4d4900e685210ea8a4106dd0bc6f4c604"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import word_tokenize\n",
      "import nltk\n",
      "from nltk.corpus import movie_reviews\n",
      "import collections"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bag_of_words(words):\n",
      "    return dict([(word, True) for word in words])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def label_feats_from_corpus(corp, feature_detector=bag_of_words):\n",
      "  label_feats = collections.defaultdict(list)\n",
      "  for label in corp.categories():\n",
      "    for fileid in corp.fileids(categories=[label]):\n",
      "      feats = feature_detector(corp.words(fileids=[fileid]))\n",
      "      label_feats[label].append(feats)\n",
      "  return label_feats\n",
      "    \n",
      "def split_label_feats(lfeats, split=0.75):\n",
      "  train_feats = []\n",
      "  test_feats = []\n",
      "  for label, feats in lfeats.items():\n",
      "    cutoff = int(len(feats) * split)\n",
      "    train_feats.extend([(feat, label) for feat in feats[:cutoff]])\n",
      "    test_feats.extend([(feat, label) for feat in feats[cutoff:]])\n",
      "  return train_feats, test_feats"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lfeats = label_feats_from_corpus(movie_reviews)\n",
      "lfeats.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "[u'neg', u'pos']"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_feats, test_feats = split_label_feats(lfeats)\n",
      "len(train_feats)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "1500"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.classify import NaiveBayesClassifier\n",
      "nb_classifier = NaiveBayesClassifier.train(train_feats)\n",
      "nb_classifier.labels()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "[u'neg', u'pos']"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "string_2007 = (\"America where is the justice where are our leaders America where can you lead us America how can you heal us America\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list_2007 = word_tokenize(string_2007)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "attempt_2007 = bag_of_words(list_2007)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nb_classifier.classify(attempt_2007)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "u'pos'"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.classify.util import accuracy\n",
      "accuracy(nb_classifier, test_feats)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "0.728"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk \n",
      "from nltk.book import *\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "*** Introductory Examples for the NLTK Book ***\n",
        "Loading text1, ..., text9 and sent1, ..., sent9\n",
        "Type the name of the text or sentence to view it.\n",
        "Type: 'texts()' or 'sents()' to list the materials.\n",
        "text1:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Moby Dick by Herman Melville 1851\n",
        "text2:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Sense and Sensibility by Jane Austen 1811\n",
        "text3:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " The Book of Genesis\n",
        "text4:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Inaugural Address Corpus\n",
        "text5:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Chat Corpus\n",
        "text6: Monty Python and the Holy Grail\n",
        "text7:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Wall Street Journal\n",
        "text8: Personals Corpus\n",
        "text9:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " The Man Who Was Thursday by G . K . Chesterton 1908\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print sys.version"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2.7.8 |Anaconda 2.0.1 (x86_64)| (default, Aug 21 2014, 15:21:46) \n",
        "[GCC 4.2.1 (Apple Inc. build 5577)]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "nltk.__version__"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "'3.0.0'"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function\n",
      "\n",
      "import itertools\n",
      "import sys\n",
      "from nltk.grammar import Nonterminal\n",
      "\n",
      "\n",
      "def generate(grammar, start=None, depth=None, n=None):\n",
      "    \"\"\"\n",
      "    Generates an iterator of all sentences from a CFG.\n",
      "\n",
      "    :param grammar: The Grammar used to generate sentences.\n",
      "    :param start: The Nonterminal from which to start generate sentences.\n",
      "    :param depth: The maximal depth of the generated tree.\n",
      "    :param n: The maximum number of sentences to return.\n",
      "    :return: An iterator of lists of terminal tokens.\n",
      "    \"\"\"\n",
      "    if not start:\n",
      "        start = grammar.start()\n",
      "    if depth is None:\n",
      "        depth = sys.maxsize\n",
      "\n",
      "    iter = _generate_all(grammar, [start], depth)\n",
      "\n",
      "    if n:\n",
      "        iter = itertools.islice(iter, n)\n",
      "\n",
      "    return iter\n",
      "\n",
      "def _generate_all(grammar, items, depth):\n",
      "    if items:\n",
      "        for frag1 in _generate_one(grammar, items[0], depth):\n",
      "            for frag2 in _generate_all(grammar, items[1:], depth):\n",
      "                yield frag1 + frag2\n",
      "    else:\n",
      "        yield []\n",
      "\n",
      "def _generate_one(grammar, item, depth):\n",
      "    if depth > 0:\n",
      "        if isinstance(item, Nonterminal):\n",
      "            for prod in grammar.productions(lhs=item):\n",
      "                for frag in _generate_all(grammar, prod.rhs(), depth-1):\n",
      "                    yield frag\n",
      "        else:\n",
      "            yield [item]\n",
      "\n",
      "demo_grammar = \"\"\"\n",
      "  S -> NP VP\n",
      "  NP -> Det N\n",
      "  PP -> P NP\n",
      "  VP -> 'slept' | 'saw' NP | 'walked' PP\n",
      "  Det -> 'the' | 'a'\n",
      "  N -> 'man' | 'park' | 'dog'\n",
      "  P -> 'in' | 'with'\n",
      "\"\"\"\n",
      "\n",
      "def demo(N=23):\n",
      "    from nltk.grammar import CFG\n",
      "\n",
      "    print('Generating the first %d sentences for demo grammar:' % (N,))\n",
      "    print(demo_grammar)\n",
      "    grammar = CFG.fromstring(demo_grammar)\n",
      "    for n, sent in enumerate(generate(grammar, n=N), 1):\n",
      "        print('%3d. %s' % (n, ' '.join(sent)))\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    demo()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Generating the first 23 sentences for demo grammar:\n",
        "\n",
        "  S -> NP VP\n",
        "  NP -> Det N\n",
        "  PP -> P NP\n",
        "  VP -> 'slept' | 'saw' NP | 'walked' PP\n",
        "  Det -> 'the' | 'a'\n",
        "  N -> 'man' | 'park' | 'dog'\n",
        "  P -> 'in' | 'with'\n",
        "\n",
        "  1. the man slept\n",
        "  2. the man saw the man\n",
        "  3. the man saw the park\n",
        "  4. the man saw the dog\n",
        "  5. the man saw a man\n",
        "  6. the man saw a park\n",
        "  7. the man saw a dog\n",
        "  8. the man walked in the man\n",
        "  9. the man walked in the park\n",
        " 10. the man walked in the dog\n",
        " 11. the man walked in a man\n",
        " 12. the man walked in a park\n",
        " 13. the man walked in a dog\n",
        " 14. the man walked with the man\n",
        " 15. the man walked with the park\n",
        " 16. the man walked with the dog\n",
        " 17. the man walked with a man\n",
        " 18. the man walked with a park\n",
        " 19. the man walked with a dog\n",
        " 20. the park slept\n",
        " 21. the park saw the man\n",
        " 22. the park saw the park\n",
        " 23. the park saw the dog\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "generate(text2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'Text' object has no attribute 'start'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-10-9b0da49c23fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-7-76383c7dc8e9>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(grammar, start, depth, n)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \"\"\"\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAttributeError\u001b[0m: 'Text' object has no attribute 'start'"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from random import choice\n",
      "# This funcion is based on _generate_all() in nltk.parse.generate\n",
      "# It therefore assumes the same import environment otherwise.\n",
      "def generate_sample(grammar, items=[\"S\"]):\n",
      "    frags = []\n",
      "    if len(items) == 1:\n",
      "        if isinstance(items[0], Nonterminal):\n",
      "            for prod in grammar.productions(lhs=items[0]):\n",
      "                frags.append(generate_sample(grammar, prod.rhs()))\n",
      "        else:\n",
      "            frags.append(items[0])\n",
      "    else:\n",
      "        # This is where we need to make our changes\n",
      "        chosen_expansion = choice(items)\n",
      "        frags.append(generate_sample,chosen_expansion)\n",
      "    return frags"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "generate_sample(text2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "['S']"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.parse"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "<module 'nltk.parse' from '/Users/olehdubno/anaconda/lib/python2.7/site-packages/nltk/parse/__init__.pyc'>"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"parse\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "invalid syntax (<ipython-input-16-d6ecbafe8d49>, line 1)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-d6ecbafe8d49>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print \"parse\"\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#NYU Natural Language Processing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gender_features(word):\n",
      "    return {'last_letter': word[-1]}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import names\n",
      "import random\n",
      "\n",
      "names = ([(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')])\n",
      "random.shuffle(names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "featuresets = [(gender_features(n),g) for (n,g) in names]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.classify import apply_features\n",
      "\n",
      "train_set = apply_features(gender_features, names[500:])\n",
      "test_set = apply_features(gender_features, names[:500])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "\n",
      "clf = nltk.NaiveBayesClassifier.train(train_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.classify(gender_features('Mirza'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "'female'"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print nltk.classify.accuracy(clf, test_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.768\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.show_most_informative_features()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Most Informative Features\n",
        "             last_letter = u'k'             male : female =     42.4 : 1.0\n",
        "             last_letter = u'a'           female : male   =     33.1 : 1.0\n",
        "             last_letter = u'f'             male : female =     15.9 : 1.0\n",
        "             last_letter = u'p'             male : female =     11.2 : 1.0\n",
        "             last_letter = u'd'             male : female =     10.1 : 1.0\n",
        "             last_letter = u'v'             male : female =      9.8 : 1.0\n",
        "             last_letter = u'o'             male : female =      9.1 : 1.0\n",
        "             last_letter = u'm'             male : female =      7.9 : 1.0\n",
        "             last_letter = u'r'             male : female =      6.6 : 1.0\n",
        "             last_letter = u'w'             male : female =      5.4 : 1.0\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}