{
 "metadata": {
  "name": "",
  "signature": "sha256:f0fc92fd97a183c5af8df7a19aa60a636957bc6a3635093d4245b9a8b3a79989"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from boilerpipe.extract import Extractor\n",
      "import nltk\n",
      "\n",
      "%matplotlib"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using matplotlib backend: MacOSX\n"
       ]
      }
     ],
     "prompt_number": 234
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "URL = 'http://rhythmsofasoul.tumblr.com/post/96953473531/thingsiwishiknew'\n",
      "\n",
      "extractor = Extractor(extractor='ArticleExtractor', url=URL)\n",
      "\n",
      "# Python code for segmentation, POS tagging and tokenization\n",
      "text = extractor.getText()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 235
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentences = nltk.sent_tokenize(text) # NLTK default sentence segmenter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 236
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_words = [nltk.word_tokenize(text) for sent in sentences] # NLTK word tokenizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 243
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_words = [nltk.pos_tag(text) for text in all_words] # NLTK POS tagger"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 217
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#second way of tokenizing the words \n",
      "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize\n",
      "words_token = word_tokenize(text) #tokenization of text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 246
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "#second way of adding POS to words\n",
      "from nltk.tag import pos_tag\n",
      "speech = pos_tag(words_token) #attributing parts of speech to words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 225
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from nltk.chunk import ne_chunk\n",
      "\n",
      "chunk = ne_chunk(pos_tag(words_token)) #chunking and NER"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 227
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist1 = FreqDist(words_token)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 247
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print fdist1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<FreqDist: u',': 62, u'you': 34, u'to': 18, u'and': 15, u'of': 15, u'is': 14, u'that': 14, u'the': 14, u'I': 12, u'it': 12, ...>\n"
       ]
      }
     ],
     "prompt_number": 250
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"all words\", len(words_token) #all words\n",
      "print \"unique words\", len(fdist1) #unique words\n",
      "print \"frequency\", len(words_token)/len(fdist1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "all words 614\n",
        "unique words 257\n",
        "frequency 2\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "sec_a = words_token\n",
      "fd = nltk.FreqDist(sec_a)\n",
      "for token in sorted(fd):\n",
      "    print fd[token], token"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 &\n",
        "62 ,\n",
        "1 -\n",
        "1 .\n",
        "3 :\n",
        "2 ;\n",
        "1 A\n",
        "1 All\n",
        "2 And\n",
        "2 Answer\n",
        "1 Be\n",
        "1 Consider\n",
        "1 Do\n",
        "1 Growth\n",
        "12 I\n",
        "2 It\n",
        "2 No\n",
        "1 Own\n",
        "1 Question\n",
        "2 Speak\n",
        "1 Squander\n",
        "1 Stylistically\n",
        "2 The\n",
        "3 There\n",
        "1 What\n",
        "6 You\n",
        "10 a\n",
        "1 abuilding\n",
        "1 admired\n",
        "1 afforded\n",
        "2 all\n",
        "3 already\n",
        "1 already.\n",
        "1 am\n",
        "5 an\n",
        "15 and\n",
        "1 appreciated\n",
        "10 are\n",
        "1 around\n",
        "1 art\n",
        "1 as\n",
        "1 ask\n",
        "1 assembly\n",
        "1 at\n",
        "1 atom\n",
        "1 away\n",
        "10 be\n",
        "4 because\n",
        "1 become\n",
        "2 being.\n",
        "1 between\n",
        "1 beyond\n",
        "2 blessing\n",
        "1 body\n",
        "1 breathe\n",
        "2 bring\n",
        "2 but\n",
        "1 by\n",
        "1 call.\n",
        "1 called\n",
        "1 calls\n",
        "1 can\n",
        "1 candidate\n",
        "1 carefully\n",
        "1 challenge\n",
        "1 challenged\n",
        "1 changing\n",
        "1 class\n",
        "1 clay\n",
        "1 clay.\n",
        "1 comes\n",
        "1 connects\n",
        "1 conviction\n",
        "1 course\n",
        "1 craft\n",
        "1 crest.\n",
        "1 delve\n",
        "1 discipline\n",
        "1 discussion\n",
        "1 distinct\n",
        "3 do\n",
        "1 does\n",
        "1 dynamic\n",
        "4 each\n",
        "1 edification.\n",
        "1 even\n",
        "2 ever\n",
        "11 every\n",
        "2 everything\n",
        "1 exam\n",
        "1 exercise\n",
        "1 expanses\n",
        "1 expansive\n",
        "1 facet\n",
        "1 fall\n",
        "1 fancy.\n",
        "1 fellow\n",
        "2 felt\n",
        "1 fiber\n",
        "1 fit\n",
        "1 fitting\n",
        "3 for\n",
        "1 formidable\n",
        "1 freeing\n",
        "1 from\n",
        "2 fully\n",
        "1 fundamental\n",
        "1 future\n",
        "2 get\n",
        "1 give\n",
        "1 go.\n",
        "1 gratitude\n",
        "1 great\n",
        "1 greatness\n",
        "1 growth\n",
        "1 growth.\n",
        "1 has\n",
        "1 here\n",
        "1 host\n",
        "1 human\n",
        "3 idea\n",
        "1 ideal\n",
        "1 if\n",
        "5 in\n",
        "1 incoming\n",
        "2 incredible\n",
        "1 intangible\n",
        "4 into\n",
        "14 is\n",
        "12 it\n",
        "2 it.\n",
        "2 journey\n",
        "1 joy.\n",
        "3 knew\n",
        "2 know\n",
        "1 learn.\u201d\n",
        "1 learning.\n",
        "1 letter\n",
        "1 life\n",
        "1 like\n",
        "2 likely\n",
        "1 line\n",
        "1 listen\n",
        "1 living\n",
        "1 magnitude\n",
        "1 make\n",
        "3 makes\n",
        "1 mark\n",
        "1 me\n",
        "1 measure.\n",
        "1 meet\n",
        "1 mind\n",
        "1 mistake\n",
        "1 mold\n",
        "2 moment.\n",
        "1 more\n",
        "1 most\n",
        "2 must\n",
        "3 need\n",
        "3 no\n",
        "4 not\n",
        "1 note\n",
        "1 notion\n",
        "15 of\n",
        "1 often\n",
        "1 once\n",
        "1 open\n",
        "1 opportunity\n",
        "1 ounce\n",
        "1 our\n",
        "1 panel\n",
        "1 passion\n",
        "2 perfect\n",
        "1 perfect.\n",
        "1 perhaps\n",
        "1 persevere.\n",
        "1 person\n",
        "1 possibly-pestering\n",
        "1 powerful\n",
        "1 precipice\n",
        "1 presence\n",
        "1 processed\n",
        "1 professor\n",
        "1 promise\n",
        "1 prompted\n",
        "1 quiet\n",
        "1 quirk\n",
        "1 reason\n",
        "1 resilience\n",
        "1 ripple\n",
        "1 ripples\n",
        "1 room\n",
        "1 same.\n",
        "1 say\n",
        "1 scribed\n",
        "1 see\n",
        "1 seemed\n",
        "1 simply\n",
        "1 sing\n",
        "1 single\n",
        "1 so\n",
        "2 some\n",
        "3 something\n",
        "2 somethings\n",
        "1 spirit.\n",
        "2 squander\n",
        "1 stand\n",
        "1 step\n",
        "1 struggle\n",
        "1 stumble\n",
        "1 than\n",
        "14 that\n",
        "1 that.\n",
        "14 the\n",
        "1 their\n",
        "1 they\n",
        "3 thing\n",
        "1 things\n",
        "1 think.\n",
        "8 this\n",
        "1 through\n",
        "1 tides\n",
        "18 to\n",
        "1 touched\n",
        "1 trip\n",
        "1 trust\n",
        "2 truth\n",
        "1 unheard.\n",
        "1 unique\n",
        "1 upon\n",
        "1 us\n",
        "6 very\n",
        "1 vibrates\n",
        "1 voice\n",
        "2 was\n",
        "1 water.\n",
        "1 waves\n",
        "1 ways\n",
        "1 we\n",
        "1 welcome\n",
        "1 well\n",
        "4 what\n",
        "1 when\n",
        "1 which\n",
        "2 whole\n",
        "7 will\n",
        "4 wish\n",
        "6 with\n",
        "1 within\n",
        "1 yes\n",
        "1 yet\n",
        "34 you\n",
        "6 you.\n",
        "7 your\n",
        "1 yourself\n",
        "1 \u201cIt\n",
        "1 \u201cperfect\u201d\n"
       ]
      }
     ],
     "prompt_number": 258
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sec_a = words_token\n",
      "fd = nltk.FreqDist(sec_a)\n",
      "for token in sorted(fd):\n",
      "    print fd[token], token"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 &\n",
        "62 ,\n",
        "1 -\n",
        "1 .\n",
        "3 :\n",
        "2 ;\n",
        "1 A\n",
        "1 All\n",
        "2 And\n",
        "2 Answer\n",
        "1 Be\n",
        "1 Consider\n",
        "1 Do\n",
        "1 Growth\n",
        "12 I\n",
        "2 It\n",
        "2 No\n",
        "1 Own\n",
        "1 Question\n",
        "2 Speak\n",
        "1 Squander\n",
        "1 Stylistically\n",
        "2 The\n",
        "3 There\n",
        "1 What\n",
        "6 You\n",
        "10 a\n",
        "1 abuilding\n",
        "1 admired\n",
        "1 afforded\n",
        "2 all\n",
        "3 already\n",
        "1 already.\n",
        "1 am\n",
        "5 an\n",
        "15 and\n",
        "1 appreciated\n",
        "10 are\n",
        "1 around\n",
        "1 art\n",
        "1 as\n",
        "1 ask\n",
        "1 assembly\n",
        "1 at\n",
        "1 atom\n",
        "1 away\n",
        "10 be\n",
        "4 because\n",
        "1 become\n",
        "2 being.\n",
        "1 between\n",
        "1 beyond\n",
        "2 blessing\n",
        "1 body\n",
        "1 breathe\n",
        "2 bring\n",
        "2 but\n",
        "1 by\n",
        "1 call.\n",
        "1 called\n",
        "1 calls\n",
        "1 can\n",
        "1 candidate\n",
        "1 carefully\n",
        "1 challenge\n",
        "1 challenged\n",
        "1 changing\n",
        "1 class\n",
        "1 clay\n",
        "1 clay.\n",
        "1 comes\n",
        "1 connects\n",
        "1 conviction\n",
        "1 course\n",
        "1 craft\n",
        "1 crest.\n",
        "1 delve\n",
        "1 discipline\n",
        "1 discussion\n",
        "1 distinct\n",
        "3 do\n",
        "1 does\n",
        "1 dynamic\n",
        "4 each\n",
        "1 edification.\n",
        "1 even\n",
        "2 ever\n",
        "11 every\n",
        "2 everything\n",
        "1 exam\n",
        "1 exercise\n",
        "1 expanses\n",
        "1 expansive\n",
        "1 facet\n",
        "1 fall\n",
        "1 fancy.\n",
        "1 fellow\n",
        "2 felt\n",
        "1 fiber\n",
        "1 fit\n",
        "1 fitting\n",
        "3 for\n",
        "1 formidable\n",
        "1 freeing\n",
        "1 from\n",
        "2 fully\n",
        "1 fundamental\n",
        "1 future\n",
        "2 get\n",
        "1 give\n",
        "1 go.\n",
        "1 gratitude\n",
        "1 great\n",
        "1 greatness\n",
        "1 growth\n",
        "1 growth.\n",
        "1 has\n",
        "1 here\n",
        "1 host\n",
        "1 human\n",
        "3 idea\n",
        "1 ideal\n",
        "1 if\n",
        "5 in\n",
        "1 incoming\n",
        "2 incredible\n",
        "1 intangible\n",
        "4 into\n",
        "14 is\n",
        "12 it\n",
        "2 it.\n",
        "2 journey\n",
        "1 joy.\n",
        "3 knew\n",
        "2 know\n",
        "1 learn.\u201d\n",
        "1 learning.\n",
        "1 letter\n",
        "1 life\n",
        "1 like\n",
        "2 likely\n",
        "1 line\n",
        "1 listen\n",
        "1 living\n",
        "1 magnitude\n",
        "1 make\n",
        "3 makes\n",
        "1 mark\n",
        "1 me\n",
        "1 measure.\n",
        "1 meet\n",
        "1 mind\n",
        "1 mistake\n",
        "1 mold\n",
        "2 moment.\n",
        "1 more\n",
        "1 most\n",
        "2 must\n",
        "3 need\n",
        "3 no\n",
        "4 not\n",
        "1 note\n",
        "1 notion\n",
        "15 of\n",
        "1 often\n",
        "1 once\n",
        "1 open\n",
        "1 opportunity\n",
        "1 ounce\n",
        "1 our\n",
        "1 panel\n",
        "1 passion\n",
        "2 perfect\n",
        "1 perfect.\n",
        "1 perhaps\n",
        "1 persevere.\n",
        "1 person\n",
        "1 possibly-pestering\n",
        "1 powerful\n",
        "1 precipice\n",
        "1 presence\n",
        "1 processed\n",
        "1 professor\n",
        "1 promise\n",
        "1 prompted\n",
        "1 quiet\n",
        "1 quirk\n",
        "1 reason\n",
        "1 resilience\n",
        "1 ripple\n",
        "1 ripples\n",
        "1 room\n",
        "1 same.\n",
        "1 say\n",
        "1 scribed\n",
        "1 see\n",
        "1 seemed\n",
        "1 simply\n",
        "1 sing\n",
        "1 single\n",
        "1 so\n",
        "2 some\n",
        "3 something\n",
        "2 somethings\n",
        "1 spirit.\n",
        "2 squander\n",
        "1 stand\n",
        "1 step\n",
        "1 struggle\n",
        "1 stumble\n",
        "1 than\n",
        "14 that\n",
        "1 that.\n",
        "14 the\n",
        "1 their\n",
        "1 they\n",
        "3 thing\n",
        "1 things\n",
        "1 think.\n",
        "8 this\n",
        "1 through\n",
        "1 tides\n",
        "18 to\n",
        "1 touched\n",
        "1 trip\n",
        "1 trust\n",
        "2 truth\n",
        "1 unheard.\n",
        "1 unique\n",
        "1 upon\n",
        "1 us\n",
        "6 very\n",
        "1 vibrates\n",
        "1 voice\n",
        "2 was\n",
        "1 water.\n",
        "1 waves\n",
        "1 ways\n",
        "1 we\n",
        "1 welcome\n",
        "1 well\n",
        "4 what\n",
        "1 when\n",
        "1 which\n",
        "2 whole\n",
        "7 will\n",
        "4 wish\n",
        "6 with\n",
        "1 within\n",
        "1 yes\n",
        "1 yet\n",
        "34 you\n",
        "6 you.\n",
        "7 your\n",
        "1 yourself\n",
        "1 \u201cIt\n",
        "1 \u201cperfect\u201d\n"
       ]
      }
     ],
     "prompt_number": 123
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print fdist1.items()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u',', 62), (u'you', 34), (u'to', 18), (u'and', 15), (u'of', 15), (u'is', 14), (u'that', 14), (u'the', 14), (u'I', 12), (u'it', 12), (u'every', 11), (u'a', 10), (u'are', 10), (u'be', 10), (u'this', 8), (u'will', 7), (u'your', 7), (u'You', 6), (u'very', 6), (u'with', 6), (u'you.', 6), (u'an', 5), (u'in', 5), (u'because', 4), (u'each', 4), (u'into', 4), (u'not', 4), (u'what', 4), (u'wish', 4), (u':', 3), (u'There', 3), (u'already', 3), (u'do', 3), (u'for', 3), (u'idea', 3), (u'knew', 3), (u'makes', 3), (u'need', 3), (u'no', 3), (u'something', 3), (u'thing', 3), (u';', 2), (u'And', 2), (u'Answer', 2), (u'It', 2), (u'No', 2), (u'Speak', 2), (u'The', 2), (u'all', 2), (u'being.', 2), (u'blessing', 2), (u'bring', 2), (u'but', 2), (u'ever', 2), (u'everything', 2), (u'felt', 2), (u'fully', 2), (u'get', 2), (u'incredible', 2), (u'it.', 2), (u'journey', 2), (u'know', 2), (u'likely', 2), (u'moment.', 2), (u'must', 2), (u'perfect', 2), (u'some', 2), (u'somethings', 2), (u'squander', 2), (u'truth', 2), (u'was', 2), (u'whole', 2), (u'&', 1), (u'-', 1), (u'.', 1), (u'A', 1), (u'All', 1), (u'Be', 1), (u'Consider', 1), (u'Do', 1), (u'Growth', 1), (u'Own', 1), (u'Question', 1), (u'Squander', 1), (u'Stylistically', 1), (u'What', 1), (u'abuilding', 1), (u'admired', 1), (u'afforded', 1), (u'already.', 1), (u'am', 1), (u'appreciated', 1), (u'around', 1), (u'art', 1), (u'as', 1), (u'ask', 1), (u'assembly', 1), (u'at', 1), (u'atom', 1), (u'away', 1), (u'become', 1), (u'between', 1), (u'beyond', 1), (u'body', 1), (u'breathe', 1), (u'by', 1), (u'call.', 1), (u'called', 1), (u'calls', 1), (u'can', 1), (u'candidate', 1), (u'carefully', 1), (u'challenge', 1), (u'challenged', 1), (u'changing', 1), (u'class', 1), (u'clay', 1), (u'clay.', 1), (u'comes', 1), (u'connects', 1), (u'conviction', 1), (u'course', 1), (u'craft', 1), (u'crest.', 1), (u'delve', 1), (u'discipline', 1), (u'discussion', 1), (u'distinct', 1), (u'does', 1), (u'dynamic', 1), (u'edification.', 1), (u'even', 1), (u'exam', 1), (u'exercise', 1), (u'expanses', 1), (u'expansive', 1), (u'facet', 1), (u'fall', 1), (u'fancy.', 1), (u'fellow', 1), (u'fiber', 1), (u'fit', 1), (u'fitting', 1), (u'formidable', 1), (u'freeing', 1), (u'from', 1), (u'fundamental', 1), (u'future', 1), (u'give', 1), (u'go.', 1), (u'gratitude', 1), (u'great', 1), (u'greatness', 1), (u'growth', 1), (u'growth.', 1), (u'has', 1), (u'here', 1), (u'host', 1), (u'human', 1), (u'ideal', 1), (u'if', 1), (u'incoming', 1), (u'intangible', 1), (u'joy.', 1), (u'learn.\\u201d', 1), (u'learning.', 1), (u'letter', 1), (u'life', 1), (u'like', 1), (u'line', 1), (u'listen', 1), (u'living', 1), (u'magnitude', 1), (u'make', 1), (u'mark', 1), (u'me', 1), (u'measure.', 1), (u'meet', 1), (u'mind', 1), (u'mistake', 1), (u'mold', 1), (u'more', 1), (u'most', 1), (u'note', 1), (u'notion', 1), (u'often', 1), (u'once', 1), (u'open', 1), (u'opportunity', 1), (u'ounce', 1), (u'our', 1), (u'panel', 1), (u'passion', 1), (u'perfect.', 1), (u'perhaps', 1), (u'persevere.', 1), (u'person', 1), (u'possibly-pestering', 1), (u'powerful', 1), (u'precipice', 1), (u'presence', 1), (u'processed', 1), (u'professor', 1), (u'promise', 1), (u'prompted', 1), (u'quiet', 1), (u'quirk', 1), (u'reason', 1), (u'resilience', 1), (u'ripple', 1), (u'ripples', 1), (u'room', 1), (u'same.', 1), (u'say', 1), (u'scribed', 1), (u'see', 1), (u'seemed', 1), (u'simply', 1), (u'sing', 1), (u'single', 1), (u'so', 1), (u'spirit.', 1), (u'stand', 1), (u'step', 1), (u'struggle', 1), (u'stumble', 1), (u'than', 1), (u'that.', 1), (u'their', 1), (u'they', 1), (u'things', 1), (u'think.', 1), (u'through', 1), (u'tides', 1), (u'touched', 1), (u'trip', 1), (u'trust', 1), (u'unheard.', 1), (u'unique', 1), (u'upon', 1), (u'us', 1), (u'vibrates', 1), (u'voice', 1), (u'water.', 1), (u'waves', 1), (u'ways', 1), (u'we', 1), (u'welcome', 1), (u'well', 1), (u'when', 1), (u'which', 1), (u'within', 1), (u'yes', 1), (u'yet', 1), (u'yourself', 1), (u'\\u201cIt', 1), (u'\\u201cperfect\\u201d', 1)]\n"
       ]
      }
     ],
     "prompt_number": 251
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "code for finding the number of unique words in El's number of words used\n",
      "\n",
      "should return a number between 0 and 1"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "import pandas as pd\n",
      "import json\n",
      "\n",
      "\n",
      "\n",
      "def word_uniqueness(all_words):\n",
      "    return len(set(words_token))/len(words_token)\n",
      "print word_uniqueness(words_token)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.418566775244\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "code to drop duplicates"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "import pandas as pd\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words = nltk.tokenize.wordpunct_tokenize(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = t\n",
      "text = text.lower()\n",
      "text = text.replace('@\\w+', text)\n",
      "text = text.replace('(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)*\\/?',text)\n",
      "text = nltk.tokenize.wordpunct_tokenize(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 254
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words_2 = [len(set(i))>3 for i in text]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "our_array = []\n",
      "for i in text:\n",
      "    if len(i) > 3:\n",
      "        our_array.append(i)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 259
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist2 = FreqDist(our_array)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 186
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print fdist2.items()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u'that', 15), (u'every', 11), (u'this', 8), (u'will', 7), (u'your', 7), (u'very', 6), (u'with', 6), (u'what', 5), (u'already', 4), (u'because', 4), (u'each', 4), (u'into', 4), (u'perfect', 4), (u'wish', 4), (u'growth', 3), (u'idea', 3), (u'knew', 3), (u'makes', 3), (u'need', 3), (u'something', 3), (u'squander', 3), (u'there', 3), (u'thing', 3), (u'answer', 2), (u'being', 2), (u'blessing', 2), (u'bring', 2), (u'clay', 2), (u'ever', 2), (u'everything', 2), (u'felt', 2), (u'fully', 2), (u'incredible', 2), (u'journey', 2), (u'know', 2), (u'likely', 2), (u'moment', 2), (u'must', 2), (u'some', 2), (u'somethings', 2), (u'speak', 2), (u'truth', 2), (u'whole', 2), (u'abuilding', 1), (u'admired', 1), (u'afforded', 1), (u'appreciated', 1), (u'around', 1), (u'assembly', 1), (u'atom', 1), (u'away', 1), (u'become', 1), (u'between', 1), (u'beyond', 1), (u'body', 1), (u'breathe', 1), (u'call', 1), (u'called', 1), (u'calls', 1), (u'candidate', 1), (u'carefully', 1), (u'challenge', 1), (u'challenged', 1), (u'changing', 1), (u'class', 1), (u'comes', 1), (u'connects', 1), (u'consider', 1), (u'conviction', 1), (u'course', 1), (u'craft', 1), (u'crest', 1), (u'delve', 1), (u'discipline', 1), (u'discussion', 1), (u'distinct', 1), (u'does', 1), (u'dynamic', 1), (u'edification', 1), (u'even', 1), (u'exam', 1), (u'exercise', 1), (u'expanses', 1), (u'expansive', 1), (u'facet', 1), (u'fall', 1), (u'fancy', 1), (u'fellow', 1), (u'fiber', 1), (u'fitting', 1), (u'formidable', 1), (u'freeing', 1), (u'from', 1), (u'fundamental', 1), (u'future', 1), (u'give', 1), (u'gratitude', 1), (u'great', 1), (u'greatness', 1), (u'here', 1), (u'host', 1), (u'human', 1), (u'ideal', 1), (u'incoming', 1), (u'intangible', 1), (u'learn', 1), (u'learning', 1), (u'letter', 1), (u'life', 1), (u'like', 1), (u'line', 1), (u'listen', 1), (u'living', 1), (u'magnitude', 1), (u'make', 1), (u'mark', 1), (u'measure', 1), (u'meet', 1), (u'mind', 1), (u'mistake', 1), (u'mold', 1), (u'more', 1), (u'most', 1), (u'note', 1), (u'notion', 1), (u'often', 1), (u'once', 1), (u'open', 1), (u'opportunity', 1), (u'ounce', 1), (u'panel', 1), (u'passion', 1), (u'perhaps', 1), (u'persevere', 1), (u'person', 1), (u'pestering', 1), (u'possibly', 1), (u'powerful', 1), (u'precipice', 1), (u'presence', 1), (u'processed', 1), (u'professor', 1), (u'promise', 1), (u'prompted', 1), (u'question', 1), (u'quiet', 1), (u'quirk', 1), (u'reason', 1), (u'resilience', 1), (u'ripple', 1), (u'ripples', 1), (u'room', 1), (u'same', 1), (u'scribed', 1), (u'seemed', 1), (u'simply', 1), (u'sing', 1), (u'single', 1), (u'spirit', 1), (u'stand', 1), (u'step', 1), (u'struggle', 1), (u'stumble', 1), (u'stylistically', 1), (u'than', 1), (u'their', 1), (u'they', 1), (u'things', 1), (u'think', 1), (u'through', 1), (u'tides', 1), (u'touched', 1), (u'trip', 1), (u'trust', 1), (u'unheard', 1), (u'unique', 1), (u'upon', 1), (u'vibrates', 1), (u'voice', 1), (u'water', 1), (u'waves', 1), (u'ways', 1), (u'welcome', 1), (u'well', 1), (u'when', 1), (u'which', 1), (u'within', 1), (u'yourself', 1)]\n"
       ]
      }
     ],
     "prompt_number": 253
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}